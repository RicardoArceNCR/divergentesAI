### app//__init__.py ###

### app//__version__.py ###
__version__ = "0.1.0"
### app//database.py ###
# app/database.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.models.db_modelos import Base  # este es el que define la tabla

SQLALCHEMY_DATABASE_URL = "sqlite:///./basedatos.db"

engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False}
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Esta línea crea las tablas si no existen
Base.metadata.create_all(bind=engine)

### app//database/__init__.py ###

### app//database/chroma_db.py ###
import chromadb
from chromadb.config import Settings

try:
    chroma_client = chromadb.Client(
        Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory="./data/chromadb"
        )
    )
except Exception as e:
    chroma_client = None
    print(f"⚠️ Error inicializando Chroma: {e}")

def get_or_create_collection(collection_name="articulos"):
    if chroma_client is None:
        raise RuntimeError("Chroma Client no está disponible.")
    return chroma_client.get_or_create_collection(name=collection_name)

def insert_documents(documents: list, ids: list, metadatas: list):
    collection = get_or_create_collection()
    collection.add(documents=documents, ids=ids, metadatas=metadatas)
    chroma_client.persist()

def query_similar_documents(query_text: str, n_results: int = 5):
    collection = get_or_create_collection()
    results = collection.query(
        query_texts=[query_text],
        n_results=n_results,
        include=["documents", "metadatas"]
    )
    return results

### app//extraction/__init__.py ###

### app//extraction/extractor_generico.py ###

### app//extraction/extractor_scraper_based.py ###

### app//extraction/extractor_utils.py ###

### app//ingestion/cleaners/__init__.py ###

### app//ingestion/cleaners/text_cleaner.py ###
import re
import unicodedata

def clean_text(text):
    text = unicodedata.normalize("NFKC", text)
    text = re.sub(r"\s+", " ", text)
    text = text.strip()
    return text

### app//ingestion/ingestion_pipeline.py ###
# app/ingestion/ingestion_pipeline.py
from app.ingestion.scrapers.scraper_registry import SCRAPER_REGISTRY

def recolectar_urls(n=5):
    urls = []
    for nombre, scraper in SCRAPER_REGISTRY.items():
        try:
            urls += scraper.obtener_urls_home(n=n)
        except Exception as e:
            print(f"Error scraper {nombre}: {e}")
    return urls


### app//ingestion/ingestion_utils.py ###

### app//ingestion/scrapers/__init__.py ###
from app.ingestion.scrapers.divergentes_scraper import DivergentesScraper
from app.ingestion.scrapers.laPrensa_scraper import LaPrensaScraper
from app.ingestion.scrapers.confidencial_scraper import ConfidencialScraper

SCRAPER_REGISTRY = {
    "divergentes": DivergentesScraper(),
    "laPrensa": LaPrensaScraper(),
    "confidencial": ConfidencialScraper(),
}

### app//ingestion/scrapers/base_scraper.py ###
# app/ingestion/scrapers/base.py
from abc import ABC, abstractmethod

class BaseScraper(ABC):
    @abstractmethod
    def obtener_urls_home(self) -> list:
        pass

    @abstractmethod
    def extraer_contenido(self, url: str) -> dict:
        pass

### app//ingestion/scrapers/confidencial_scraper.py ###
from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin
from app.ingestion.scrapers.base_scraper import BaseScraper

class ConfidencialScraper(BaseScraper):
    def __init__(self, base_url="https://confidencial.digital"):
        self.base_url = base_url

    def obtener_urls_home(self) -> list:
        response = requests.get(self.base_url)
        soup = BeautifulSoup(response.text, "html.parser")
        urls = set()

        for a in soup.find_all("a", href=True):
            full_url = urljoin(self.base_url, a["href"])
            if self.base_url in full_url:
                urls.add(full_url)

        return [
            url for url in urls
            if ("/nacion/" in url or "/politica/" in url)
            and not any(x in url for x in ["etiqueta", "video", "opinion"])
        ]

    def extraer_contenido(self, url: str) -> dict:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        titulo_tag = soup.find("h1")
        titulo = titulo_tag.get_text(strip=True) if titulo_tag else "Sin título"

        subtitulo_tag = soup.find("h2")
        subtitulo = subtitulo_tag.get_text(strip=True) if subtitulo_tag else ""

        autor_tag = soup.find("span", class_="autor")
        autor = autor_tag.get_text(strip=True).replace("Por ", "") if autor_tag else ""

        fecha_tag = soup.find("time")
        fecha = fecha_tag.get("datetime") if fecha_tag else ""

        contenido_div = soup.find("div", class_="td-post-content") or soup.find("article")
        if not contenido_div:
            print(f"⚠️ No se encontró contenido en: {url}")
            return {}

        parrafos = contenido_div.find_all("p")
        texto = "\n".join(p.get_text(strip=True) for p in parrafos if p.get_text(strip=True))

        return {
            "titulo": titulo,
            "subtitulo": subtitulo,
            "autor": autor,
            "fecha": fecha,
            "texto": texto
        }

### app//ingestion/scrapers/divergentes_scraper.py ###
# Scraper para Divergentes robusto
# app/ingestion/scrapers/divergentes_scraper.py

from app.ingestion.scrapers.scraper_template import ScraperTemplate
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from typing import List

class DivergentesScraper(ScraperTemplate):
    def __init__(self):
        super().__init__(base_url="https://www.divergentes.com")

    def es_articulo(self, url: str) -> bool:
        url = url.split("#")[0]

        if not url.startswith(self.base_url):
            return False

        path = url.replace(self.base_url, "")
        path = path.strip("/")

        if not path:
            return False
        if path in ["categoria", "etiqueta", "opinion", "podcast", "investigaciones"]:
            return False
        if any(x in path for x in ["#", "?", "search", "tag"]):
            return False

        return path.count("/") >= 1

    def obtener_urls_home(self, n: int = 30) -> List[str]:
        urls = []
        try:
            response = self.scraper.get(self.base_url, timeout=self.timeout)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # Ignorar links del menú hamburguesa
            sidenav = soup.find("aside", id="sidenavmenu")
            links_hamburguesa = {urljoin(self.base_url, a["href"]) for a in sidenav.find_all("a", href=True)} if sidenav else set()

            # Ignorar links del navbar principal
            nav = soup.find("nav")
            links_navbar = {urljoin(self.base_url, a["href"]) for a in nav.find_all("a", href=True)} if nav else set()

            for link in soup.find_all("a", href=True):
                full_url = urljoin(self.base_url, link["href"])
                if full_url not in links_hamburguesa and full_url not in links_navbar:
                    # Aquí ya no filtramos por "es_articulo"
                    urls.append(full_url)

            urls = list(dict.fromkeys(urls))  # Eliminamos duplicados
            urls = urls[:n]  # Limitar solo si n está definido

        except Exception as e:
            print(f"⚠️ Error obteniendo URLs de Divergentes: {e}")

        return urls

### app//ingestion/scrapers/laPrensa_scraper.py ###
from bs4 import BeautifulSoup
import requests
from app.ingestion.scrapers.base_scraper import BaseScraper

class LaPrensaScraper(BaseScraper):
    def extraer_contenido(self, url: str) -> dict:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")

        # Título
        titulo_tag = soup.find("h1", class_="title")
        titulo = titulo_tag.get_text(strip=True) if titulo_tag else "Sin título"

        # Subtítulo (si existe)
        subtitulo_tag = soup.find("div", class_="sub-title")
        subtitulo = subtitulo_tag.get_text(strip=True) if subtitulo_tag else ""

        # Autor
        autor_tag = soup.find("span", class_="autor")
        autor = autor_tag.get_text(strip=True).replace("Por ", "") if autor_tag else ""

        # Fecha
        fecha_tag = soup.find("span", class_="td-post-date")
        fecha = fecha_tag.get_text(strip=True) if fecha_tag else ""

        # Contenido
        contenido_div = soup.find("div", class_="td-post-content tagdiv-type")
        if not contenido_div:
            print(f"⚠️ No se encontró contenido en: {url}")
            return {}

        parrafos = contenido_div.find_all("p")
        texto = "\n".join(p.get_text(strip=True) for p in parrafos if p.get_text(strip=True))

        return {
            "titulo": titulo,
            "subtitulo": subtitulo,
            "autor": autor,
            "fecha": fecha,
            "texto": texto
        }

    def obtener_urls_home(self):
        # Implementalo o dejalo pasar por ahora
        return []

### app//ingestion/scrapers/scraper_registry.py ###

### app//ingestion/scrapers/scraper_template.py ###
# app/ingestion/scrapers/scraper_template.py

from abc import ABC, abstractmethod
from typing import List, Dict
import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import logging

class BaseScraper(ABC):
    @abstractmethod
    def obtener_urls_home(self, n: int = 2) -> List[str]:
        pass

    @abstractmethod
    def extraer_contenido(self, url: str) -> Dict[str, str]:
        pass

class ScraperTemplate(BaseScraper):
    TEXTOS_BASURA = [
        "Todos los derechos reservados",
        "Esta publicación puede ser utilizada",
        "previa autorización de la dirección del medio",
        "[email protected]",
        "Mostrar más resultados"
    ]

    def __init__(self, base_url: str):
        self.base_url = base_url
        self.scraper = cloudscraper.create_scraper()
        self.timeout = 10  # segundos

    def es_articulo(self, url: str) -> bool:
        return (
            url.strip("/").count("/") >= 2
            and not any(x in url for x in ["opinion", "podcast", "etiqueta", "categoria"])
        )

    def obtener_urls_home(self, n: int = 5) -> List[str]:
        urls = []
        try:
            response = self.scraper.get(self.base_url, timeout=self.timeout)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            links = soup.find_all("a", href=True)

            for link in links:
                full_url = urljoin(self.base_url, link["href"])
                if self.es_articulo(full_url) and full_url not in urls:
                    urls.append(full_url)
                if len(urls) >= n:
                    break
        except Exception as e:
            logging.error(f"Error al obtener URLs del home: {e}")
        return urls

    def extraer_contenido(self, url: str) -> Dict[str, str]:
        contenido = {"titulo": "", "texto": ""}
        try:
            response = self.scraper.get(url, timeout=self.timeout)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            titulo_tag = soup.find("h1")
            texto_tags = soup.find_all(["p", "div"], recursive=True)

            if titulo_tag:
                contenido["titulo"] = titulo_tag.get_text(strip=True)

            texto_final = []
            for tag in texto_tags:
                if tag.name == "p" and tag.get_text(strip=True):
                    texto = tag.get_text(strip=True)
                    if not any(basura in texto for basura in self.TEXTOS_BASURA):
                        texto_final.append(texto)

            contenido["texto"] = " ".join(texto_final)

        except Exception as e:
            logging.error(f"Error extrayendo contenido de {url}: {e}")
        
        return contenido

### app//logic/analizador.py ###
# app/logic/analizador.py
from app.extraction.extractor_scraper_based import extraer_contenido
from app.logic.summary import resumir
from app.logic.classification import clasificar_texto
from app.logic.entidades import extraer_entidades
from app.services.embedding_utils import generar_embedding
from app.utils.error_handler import manejar_error

class AnalizadorArticulo:
    def __init__(self, url: str):
        self.url = url
        self.datos = {}
        self.resultado = {}

    def extraer(self):
        try:
            self.datos = extraer_contenido(self.url)
        except Exception as e:
            manejar_error(f"Error al extraer: {self.url}", e)

    def analizar(self):
        if not self.datos.get("texto"):
            manejar_error("Texto vacío", Exception())
            return

        texto = self.datos["texto"]

        self.resultado = {
            "titulo": self.datos.get("titulo", "Sin título"),
            "subtitulo": self.datos.get("subtitulo", ""),
            "autor": self.datos.get("autor", ""),
            "fecha": self.datos.get("fecha", ""),
            "url": self.url,
            "texto": texto,
            "resumen": resumir(texto),
            "categorias": clasificar_texto(texto),
            "entidades": extraer_entidades(texto),
            "embedding": generar_embedding(texto),
        }

    def ejecutar(self):
        self.extraer()
        self.analizar()
        return self.resultado

### app//logic/classification.py ###

### app//logic/entidades.py ###
import spacy

nlp = spacy.load("es_core_news_sm")

def extraer_entidades(texto: str) -> list:
    doc = nlp(texto)
    return [{"texto": ent.text, "etiqueta": ent.label_} for ent in doc.ents]

### app//logic/summary.py ###
# app/logic/summary.py

def resumir(texto: str) -> str:
    # Esta es una versión simplificada para pruebas
    return texto[:300] + "..." if len(texto) > 300 else texto

### app//main.py ###
# app/main.py
from fastapi import FastAPI
from app.routes import articles, summaries, classification_router
from app.routes import query_router, upsert_router, resumen_router  # 👈 nuevos routers RAG

def create_app() -> FastAPI:
    app = FastAPI(title="DivergenteRAG 2.0")

    # Routers antiguos (ya en tu sistema)
    app.include_router(articles.router)
    app.include_router(summaries.router)
    app.include_router(classification_router.router)

    # Routers nuevos para RAG
    app.include_router(query_router.router)
    app.include_router(upsert_router.router)
    app.include_router(resumen_router.router)

    return app

app = create_app()
### app//models/__init__.py ###

### app//models/article.py ###
from pydantic import BaseModel, Field, validator, HttpUrl
from typing import Optional, List


# 🧾 Entradas simples

class URLInput(BaseModel):
    url: HttpUrl = Field(..., description="URL del artículo o recurso a procesar")

class TextoInput(BaseModel):
    texto: str = Field(..., description="Texto libre para análisis o clasificación")

class TextoResumenInput(BaseModel):
    texto: str = Field(
        ...,
        example="El presidente fue acusado de soborno...",
        description="Texto a resumir"
    )


# 🧠 Salidas generales

class TextoOutput(BaseModel):
    resumen: str = Field(..., description="Resumen generado del texto enviado")


# 📰 Artículo base

class Articulo(BaseModel):
    titulo: str = Field(..., description="Título principal del artículo")
    resumen: Optional[str] = Field(None, description="Resumen automático del contenido")
    url: HttpUrl = Field(..., description="Enlace original del artículo")
    autor: Optional[str] = Field(None, description="Nombre del autor (si está disponible)")
    fecha: Optional[str] = Field(None, description="Fecha de publicación (si está disponible)")
    embedding: Optional[List[float]] = Field(
        None,
        description="Vector de representación semántica del artículo",
        example=[0.123, -0.456, 0.789]
    )

    class Config:
        from_attributes = True
        json_schema_extra = {
            "example": {
                "titulo": "Protestas sacuden la capital",
                "resumen": "Se registraron protestas masivas...",
                "url": "https://ejemplo.com/articulo",
                "autor": "Juan Pérez",
                "fecha": "2025-04-20",
                "embedding": [0.123, -0.456, 0.789]
            }
        }


# 🧩 Artículo extendido

class ArticuloExtendido(Articulo):
    subtitulo: Optional[str] = Field(None, description="Subtítulo del artículo")
    texto: str = Field(..., description="Cuerpo completo del artículo")

    links_relacionados: Optional[List[str]] = Field(default_factory=list, description="Enlaces internos del sitio")
    links_externos: Optional[List[str]] = Field(default_factory=list, description="Enlaces a otros medios o recursos")
    documentos: Optional[List[str]] = Field(default_factory=list, description="Documentos enlazados (PDF, DOCX, etc.)")
    apis: Optional[List[str]] = Field(default_factory=list, description="Enlaces relacionados con APIs detectadas")
    anuncios: Optional[List[str]] = Field(default_factory=list, description="Fragmentos de texto identificados como anuncios")
    colores: Optional[List[str]] = Field(default_factory=list, description="Colores CSS identificados en estilos inline")

    class Config:
        from_attributes = True
        json_schema_extra = {
            "example": {
                "titulo": "Protestas sacuden la capital",
                "subtitulo": "Grupos civiles se enfrentan con la policía",
                "texto": "El conflicto comenzó luego de...",
                "resumen": "Se registraron protestas masivas...",
                "url": "https://ejemplo.com/articulo",
                "autor": "Juan Pérez",
                "fecha": "2025-04-20",
                "links_relacionados": ["https://ejemplo.com/relacionado1"],
                "links_externos": ["https://medioexterno.com/nota"],
                "documentos": ["https://ejemplo.com/archivo.pdf"],
                "apis": ["https://api.ejemplo.com/endpoint"],
                "anuncios": ["Publicidad 1"],
                "colores": ["#123456", "#abcdef"],
                "embedding": [0.123, -0.456, 0.789]
            }
        }

### app//models/article_db.py ###

### app//models/base.py ###

### app//models/classification.py ###
from pydantic import BaseModel, Field
from typing import Dict

class ClassificationOutput(BaseModel):
    categorias: Dict[str, float] = Field(
        ...,
        example={
            "política": 0.95,
            "corrupción": 0.8,
            "derechos_humanos": 0.5
        },
        description="Temas detectados en el texto con su nivel de relevancia"
    )

### app//models/db_modelos.py ###
# app/modelos/db_modelos.py
from sqlalchemy import Column, Integer, String, Text, DateTime, JSON
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

Base = declarative_base()

class ResultadoProcesado(Base):
    __tablename__ = "resultados"

    id = Column(Integer, primary_key=True, index=True)
    url = Column(String)
    titulo = Column(String)
    resumen = Column(Text)
    categorias = Column(JSON)  # ej: {"politica": 0.95}
    fecha_procesado = Column(DateTime, default=datetime.utcnow)

### app//models/entities.py ###

### app//models/schemas.py ###
from pydantic import BaseModel
from typing import Optional, Dict

# Para /resumir
class ResumenInput(BaseModel):
    texto: str

# Para /upsert
class UpsertInput(BaseModel):
    titulo: str
    texto: str
    url: str

### app//models/user.py ###
from pydantic import BaseModel, EmailStr, Field
from typing import Optional

class UserBase(BaseModel):
    username: str = Field(..., description="Nombre de usuario único")
    email: EmailStr = Field(..., description="Correo electrónico válido")

class UserCreate(UserBase):
    password: str = Field(..., min_length=6, description="Contraseña segura (mínimo 6 caracteres)")

class UserLogin(BaseModel):
    email: EmailStr = Field(..., description="Correo electrónico del usuario")
    password: str = Field(..., description="Contraseña del usuario")

class UserResponse(UserBase):
    id: int = Field(..., description="ID único del usuario")
    is_active: bool = Field(..., description="Indica si el usuario está activo")

    class Config:
        from_attributes = True

### app//nlp/clasificador.py ###
import warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="huggingface_hub")

from sentence_transformers import SentenceTransformer, util

modelo = SentenceTransformer("all-MiniLM-L6-v2")

CATEGORIAS = {
    "corrupción": "Este texto trata sobre corrupción política o institucional.",
    "política": "Este texto trata sobre decisiones gubernamentales, partidos políticos o procesos electorales.",
    "violencia": "Este texto menciona represión, asesinatos, uso de la fuerza o violencia policial.",
    "economía": "Este texto trata sobre dinero, finanzas, impuestos o economía del país.",
    "derechos humanos": "Este texto trata sobre violaciones a los derechos fundamentales.",
    "internacional": "Este texto se relaciona con relaciones exteriores o actores internacionales."
}

def clasificar_texto(texto: str, top_n: int = 3) -> dict:
    texto_emb = modelo.encode(texto, convert_to_tensor=True)
    scores = {}
    for categoria, descripcion in CATEGORIAS.items():
        desc_emb = modelo.encode(descripcion, convert_to_tensor=True)
        score = util.cos_sim(texto_emb, desc_emb).item()
        scores[categoria] = round(score, 3)

    return dict(sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n])

### app//nlp/embedding.py ###
from sentence_transformers import SentenceTransformer
import json

modelo = SentenceTransformer("all-MiniLM-L6-v2")

def generar_embedding(texto: str) -> list:
    return modelo.encode(texto).tolist()

def serializar_embedding(embedding: list) -> str:
    return json.dumps(embedding)

def deserializar_embedding(embedding_str: str) -> list:
    return json.loads(embedding_str)

### app//nlp/embedding_model.py ###

### app//nlp/entidades_extractores.py ###
import spacy
nlp = spacy.load("es_core_news_md")

def extraer_entidades(texto: str) -> dict:
    doc = nlp(texto)
    entidades = {"PERSONA": [], "ORG": [], "LOC": []}
    for ent in doc.ents:
        if ent.label_ in entidades:
            entidades[ent.label_].append(ent.text)
    return {k: list(set(v)) for k, v in entidades.items()}
### app//nlp/resumen.py ###

### app//routes/__init__.py ###

### app//routes/analysis.py ###
from fastapi import APIRouter
from app.services.scrapers.coordinador_scrapers import obtener_urls_home
from app.logic.analisis import procesar_articulo_completo
import os
import json
from datetime import datetime

router = APIRouter()

@router.get("/articulos/analizados", tags=["Análisis completo"])
def analizar_articulos(n: int = 15):
    urls = obtener_urls_home(n=n)
    resultados = []

    for url in urls:
        print(f"📰 Analizando: {url}")
        resultado = procesar_articulo_completo(url)
        if resultado:
            resultados.append(resultado)

            # Guardar log del resultado
            nombre_archivo = url.strip("/").split("/")[-1][:50] or "sin-nombre"
            timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
            ruta = f"logs/articulos/{nombre_archivo}-{timestamp}.json"

            os.makedirs("logs/articulos", exist_ok=True)
            with open(ruta, "w") as f:
                json.dump(resultado, f, indent=2, ensure_ascii=False)

    return resultados

### app//routes/articles.py ###
# Standard Library
import logging

# FastAPI
from fastapi import APIRouter

# Internal Logic
from app.logic.summary import resumir
from app.services.scrapers.coordinador_scrapers import obtener_urls_home
from app.services.scraper_extractor import extraer_contenido
from app.services.sitemap_scraper import obtener_urls_sitemap
from app.services.nlp_utils.embedding_utils import generar_embedding

# Models
from app.models.article import Articulo, ArticuloExtendido

logger = logging.getLogger(__name__)
router = APIRouter()

def procesar_url(url: str, extendido: bool = False):
    try:
        data = extraer_contenido(url)
        resumen = resumir(data["texto"])
        embedding = generar_embedding(data["texto"])

        if extendido:
            return ArticuloExtendido(
                titulo=data.get("titulo", ""),
                subtitulo=data.get("subtitulo", ""),
                texto=data.get("texto", ""),
                resumen=resumen,
                url=url,
                autor=data.get("autor", ""),
                fecha=data.get("fecha", ""),
                links_relacionados=data.get("links_relacionados", []),
                links_externos=data.get("links_externos", []),
                documentos=data.get("documentos", []),
                apis=data.get("apis", []),
                anuncios=data.get("anuncios", []),
                colores=data.get("colores", []),
                embedding=embedding
            )
        else:
            return Articulo(
                titulo=data["titulo"],
                resumen=resumen,
                url=url,
                autor=data.get("autor", ""),
                fecha=data.get("fecha", ""),
                embedding=embedding
            )
    except Exception as e:
        logger.warning(f"Error con {url}: {e}")
        return None

@router.get("/articulos", response_model=list[Articulo], tags=["Artículos"], summary="Obtener artículos resumidos con embedding")
def obtener_articulos(n: int = 30):
    urls = obtener_urls_home(n=n)
    return [a for a in (procesar_url(url) for url in urls) if a]

@router.get("/articulos/extendidos", response_model=list[ArticuloExtendido])
def articulos_ext(n: int = 30):
    urls = obtener_urls_home(n=n)
    return [a for a in (procesar_url(url, extendido=True) for url in urls) if a]

@router.get("/articulos_sitemap", response_model=list[Articulo], tags=["Artículos"])
def obtener_articulos_sitemap(n: int = 30):
    urls = obtener_urls_sitemap(n=n)
    return [a for a in (procesar_url(url) for url in urls) if a]

### app//routes/classification_router.py ###
from fastapi import APIRouter
from app.models.article import TextoResumenInput
from app.models.classification import ClassificationOutput
from app.nlp.clasificador import clasificar_texto  # <-- clasificación temática
from app.nlp.entidades_extractores import extraer_entidades

router = APIRouter()

@router.post(
    "/clasificar",
    response_model=ClassificationOutput,
    tags=["Clasificación"],
    summary="Clasificar texto por temática",
    responses={
        200: {
            "description": "Resultado de clasificación por temática",
            "content": {
                "application/json": {
                    "example": {
                        "categorias": {
                            "política": 0.75,
                            "corrupción": 0.33,
                            "derechos_humanos": 0.5
                        }
                    }
                }
            }
        }
    }
)
def clasificar(data: TextoResumenInput):
    """
    Clasifica un texto recibido por temática utilizando embeddings semánticos.
    """
    resultado = clasificar_texto(data.texto)
    return {"categorias": resultado}


@router.post(
    "/entidades",
    tags=["Clasificación"],
    summary="Extraer entidades nombradas del texto",
    responses={
        200: {
            "description": "Entidades extraídas: personas, organizaciones y lugares",
            "content": {
                "application/json": {
                    "example": {
                        "entidades": {
                            "PERSONA": ["Daniel Ortega", "Rosario Murillo"],
                            "ORG": ["Gobierno de Nicaragua"],
                            "LOC": ["Managua"]
                        }
                    }
                }
            }
        }
    }
)
def entidades(data: TextoResumenInput):
    """
    Extrae entidades nombradas del texto utilizando spaCy (personas, organizaciones, lugares).
    """
    resultado = extraer_entidades(data.texto)
    return {"entidades": resultado}

### app//routes/images.py ###
from fastapi import APIRouter
from app.services.openai.openai_client import generar_imagen

router = APIRouter()

@router.get("/imagen", tags=["Imágenes"], summary="Generar imagen desde prompt")
def obtener_imagen(prompt: str):
    url = generar_imagen(prompt)
    return {"imagen_url": url}

### app//routes/ingestion_router.py ###

### app//routes/logs.py ###
from fastapi import APIRouter
import os
import json
from app.models.article import ArticuloExtendido  # usar modelo extendido

router = APIRouter()

@router.get("/logs/resumen", response_model=list[ArticuloExtendido], tags=["Logs"])
def resumen_logs(n: int = 10):
    carpeta = "logs/articulos"
    if not os.path.exists(carpeta):
        return []

    files = sorted(os.listdir(carpeta), reverse=True)[:n]
    resultados = []

    for file in files:
        path = os.path.join(carpeta, file)
        with open(path, encoding="utf-8") as f:
            try:
                datos = json.load(f)
                resultados.append(datos)
            except Exception as e:
                print(f"⚠️ Error al leer {file}: {e}")

    return resultados

### app//routes/query_router.py ###
# app/routes/query_router.py
from fastapi import APIRouter
from app.services.rag_query_services import buscar_articulos_similares

router = APIRouter()

@router.post("/query", tags=["RAG"], summary="Buscar artículos similares")
def consultar_articulos(pregunta: str, n: int = 5):
    """
    Recibe una pregunta y devuelve artículos similares basados en embeddings.
    """
    resultados = buscar_articulos_similares(pregunta, n)
    return {"resultados": resultados}

### app//routes/resultados.py ###
# app/rutas/resultados.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
from app.models.db_modelos import ResultadoProcesado
from app.schemas import ResultadoCreate  # lo definimos abajo

router = APIRouter()

@router.post("/resultados")
def guardar_resultado(data: ResultadoCreate, db: Session = Depends(get_db)):
    nuevo = ResultadoProcesado(**data.dict())
    db.add(nuevo)
    db.commit()
    db.refresh(nuevo)
    return nuevo

@router.get("/resultados")
def listar_resultados(db: Session = Depends(get_db)):
    return db.query(ResultadoProcesado).all()

### app//routes/resumen_router.py ###
# app/routes/resumen_router.py

from fastapi import APIRouter
from app.models.schemas import ResumenInput
from summarizer import Summarizer

router = APIRouter()

# Instancia única del modelo BERT
bert_model = Summarizer()

@router.post("/resumir", tags=["Resúmenes"], summary="Resumir un texto automáticamente")
def resumir_texto(data: ResumenInput):
    """
    Recibe un texto y devuelve un resumen automático usando BERT extractivo, adaptado al tamaño del texto.
    """
    texto_original = data.texto.strip()
    longitud_texto = len(texto_original)

    if longitud_texto < 500:
        # No tiene sentido resumir textos muy pequeños
        resumen_generado = texto_original
    elif 500 <= longitud_texto <= 1500:
        # Resumen más corto
        resumen_generado = bert_model(texto_original, min_length=30, max_length=100)
    else:
        # Resumen más extenso para textos largos
        resumen_generado = bert_model(texto_original, min_length=100, max_length=300)

    return {"resumen": resumen_generado}

### app//routes/summaries.py ###
from fastapi import APIRouter
from app.logic.summary import resumir
from app.schemas import TextoResumenInput, TextoOutput

router = APIRouter()

@router.post("/resumir", response_model=TextoOutput, tags=["Resúmenes"], summary="Resumir texto enviado")
def resumir_texto(data: TextoResumenInput):
    resumen = resumir(data.texto)
    return {"resumen": resumen}

### app//routes/upsert_router.py ###
from fastapi import APIRouter
from app.models.schemas import UpsertInput  # 👈 importar el modelo correcto
from app.services.rag_query_services import agregar_articulo_a_chroma

router = APIRouter()

@router.post("/upsert", tags=["RAG"], summary="Agregar artículo manualmente")
def insertar_articulo(data: UpsertInput):
    """
    Permite agregar un nuevo artículo a la base RAG enviando título, texto y url.
    """
    respuesta = agregar_articulo_a_chroma(data.titulo, data.texto, data.url)
    return respuesta

### app//schemas.py ###
from pydantic import BaseModel
from typing import Optional, Dict

class TextoResumenInput(BaseModel):
    texto: str
    titulo: Optional[str] = "Texto sin título"
    url: Optional[str] = "manual"

class TextoOutput(BaseModel):
    resumen: str

### app//services/__init__.py ###

### app//services/nlp_utils/embedding_utils.py ###
from sentence_transformers import SentenceTransformer
import json

modelo = SentenceTransformer("all-MiniLM-L6-v2")

def generar_embedding(texto: str) -> list:
    """Devuelve un embedding como lista de floats"""
    return modelo.encode(texto).tolist()

def serializar_embedding(embedding: list) -> str:
    """Convierte la lista de floats a un string JSON"""
    return json.dumps(embedding)

def deserializar_embedding(embedding_str: str) -> list:
    """Convierte el string JSON a lista de floats"""
    return json.loads(embedding_str)

### app//services/nlp_utils/prompt_utils.py ###
def clasificar_texto(texto: str) -> dict:
    # Simulación simple basada en palabras clave
    categorias = {
        "politica": ["gobierno", "presidente", "elección", "diputado"],
        "corrupcion": ["soborno", "lavado", "malversación"],
        "derechos_humanos": ["represión", "protesta", "exilio", "presos"],
    }

    resultado = {}
    texto_lower = texto.lower()
    for categoria, palabras in categorias.items():
        resultado[categoria] = sum(p in texto_lower for p in palabras) / len(palabras)

    return resultado

### app//services/openai/openai_client.py ###
import openai
import os
from dotenv import load_dotenv

load_dotenv()

openai.api_key = os.getenv("OPENAI_API_KEY")
print("🔐 API Key (inicio parcial):", openai.api_key[:8])

def generar_imagen(prompt: str) -> str:
    """
    Genera una imagen a partir de un prompt usando OpenAI DALL·E.

    Args:
        prompt (str): Texto descriptivo para generar la imagen.

    Returns:
        str: URL de la imagen generada o un placeholder si falla.
    """
    if not prompt or not isinstance(prompt, str):
        raise ValueError("El prompt debe ser un string no vacío.")

    try:
        response = openai.images.generate(
            model="dall-e-3",  # o "dall-e-2" si tu cuenta no tiene acceso
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1,
        )
        return response.data[0].url
    except Exception as e:
        print(f"❌ Error al generar imagen: {e}")
        return "https://via.placeholder.com/1024?text=Error+al+generar+imagen"
if os.getenv("DEBUG") == "1":
    print("🔐 API Key (inicio parcial):", openai.api_key[:8])

### app//services/rag_query_services.py ###
# app/services/rag_query_service.py
from app.database.chroma_db import insert_documents, query_similar_documents
from typing import List

def agregar_articulo_a_chroma(titulo: str, texto: str, url: str):
    """
    Inserta un nuevo artículo en ChromaDB.
    """
    document = texto
    id_unico = url  # Asumimos que el URL es único
    metadata = {"titulo": titulo, "url": url}

    insert_documents(
        documents=[document],
        ids=[id_unico],
        metadatas=[metadata]
    )
    return {"mensaje": "Artículo agregado exitosamente."}

def buscar_articulos_similares(pregunta: str, n: int = 5) -> List[dict]:
    """
    Busca artículos similares a la pregunta planteada.
    """
    resultados = query_similar_documents(query_text=pregunta, n_results=n)

    documentos = resultados.get("documents", [[]])[0]
    metadatos = resultados.get("metadatas", [[]])[0]

    respuesta = []
    for doc, meta in zip(documentos, metadatos):
        respuesta.append({
            "titulo": meta.get("titulo", "Sin título"),
            "url": meta.get("url", "Desconocido"),
            "fragmento": doc[:500] + "..." if len(doc) > 500 else doc
        })

    return respuesta

### app//services/scraper_extractor.py ###
from urllib.parse import urlparse, urljoin
from app.ingestion.scrapers import SCRAPER_REGISTRY
import requests
from bs4 import BeautifulSoup

def extraer_contenido(url: str) -> dict:
    dominio = urlparse(url).netloc.lower()

    for nombre, scraper in SCRAPER_REGISTRY.items():
        if nombre.lower() in dominio:
            print(f"🧠 usando scraper personalizado: {nombre}")
            return scraper.extraer_contenido(url)

    print("🧠 usando extractor genérico")
    return extractor_generico(url)


def extractor_generico(url: str) -> dict:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    dominio = urlparse(url).netloc

    contenido = soup.select_one("article") or soup.select_one("div.entry-content") or soup
    cuerpo = "\n".join([p.get_text(strip=True) for p in contenido.find_all("p")]) if contenido else ""

    def texto_de(selector, attr=None):
        el = soup.select_one(selector)
        if el:
            return el.get(attr) if attr else el.get_text(strip=True)
        return ""

    def extraer_links(externos=False):
        links = []
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if externos:
                if dominio not in href and href.startswith("http"):
                    links.append(href)
            else:
                if dominio in href or href.startswith("/"):
                    links.append(urljoin(url, href))
        return list(set(links))

    def extraer_colores():
        colores = set()
        for tag in soup.find_all(style=True):
            estilo = tag["style"]
            for parte in estilo.split(";"):
                if "color" in parte:
                    try:
                        colores.add(parte.split(":")[1].strip())
                    except IndexError:
                        continue
        return list(colores)

    def extraer_documentos():
        return [a["href"] for a in soup.find_all("a", href=True) if any(a["href"].endswith(ext) for ext in [".pdf", ".docx", ".xlsx"])]

    def extraer_apis():
        return [a["href"] for a in soup.find_all("a", href=True) if "api" in a["href"]]

    def extraer_anuncios():
        return [div.text.strip() for div in soup.find_all(True, class_=lambda c: c and "ad" in c.lower())]

    titulo = texto_de("h1")
    subtitulo = texto_de("h2")
    autor = texto_de(".author") or texto_de('[name=author]', attr="content")
    fecha = texto_de("time", attr="datetime") or texto_de("meta[name=date]", attr="content")

    return {
        "titulo": titulo,
        "subtitulo": subtitulo,
        "autor": autor,
        "fecha": fecha,
        "texto": cuerpo,
        "url": url,
        "links_relacionados": extraer_links(externos=False),
        "links_externos": extraer_links(externos=True),
        "documentos": extraer_documentos(),
        "apis": extraer_apis(),
        "anuncios": extraer_anuncios(),
        "colores": extraer_colores()
    }

### app//services/scrapers/coordinador_scrapers.py ###
from app.ingestion.scrapers.divergentes_scraper import DivergentesScraper

# Registro global de scrapers
SCRAPER_REGISTRY = {
    "divergentes": DivergentesScraper()
}

def obtener_urls_home(n=5) -> list[str]:
    """
    Itera sobre los scrapers registrados y devuelve una lista combinada de URLs de artículos.
    """
    urls = []

    for nombre, scraper in SCRAPER_REGISTRY.items():
        try:
            urls += scraper.obtener_urls_home()[:n]
        except Exception as e:
            print(f"❌ Error al obtener URLs de {nombre}: {e}")

    return urls

### app//services/sitemap_scraper.py ###
import requests
from bs4 import BeautifulSoup
from typing import List

def obtener_urls_sitemap(base_url: str = "https://www.divergentes.com/sitemap_index.xml", n: int = 50) -> List[str]:
    urls = []
    try:
        # Leer el índice de sitemaps
        response = requests.get(base_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        # Extraer todos los sitemaps secundarios
        sitemap_links = [loc.get_text(strip=True) for loc in soup.find_all("loc")]

        for sitemap_url in sitemap_links:
            sitemap_response = requests.get(sitemap_url, timeout=10)
            sitemap_response.raise_for_status()
            sitemap_soup = BeautifulSoup(sitemap_response.content, "xml")

            # Extraer URLs reales de artículos
            for loc in sitemap_soup.find_all("loc"):
                articulo_url = loc.get_text(strip=True)
                urls.append(articulo_url)
                if len(urls) >= n:
                    break

            if len(urls) >= n:
                break

        urls = list(dict.fromkeys(urls))  # Elimina duplicados
        return urls

    except Exception as e:
        print(f"⚠️ Error leyendo sitemap: {e}")
        return []

### app//services/test_script.py ###

### app//utils/error_handler.py ###
# app/utils/error_handler.py
import logging

logger = logging.getLogger(__name__)

def manejar_error(mensaje: str, error: Exception):
    logger.error(f"{mensaje} - {str(error)}")

### app//utils/logger.py ###
# app/utils/logger.py
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

### app//utils/logs.py ###
import os
import json
from tabulate import tabulate

def resumen_logs(n: int = 10):
    carpeta = "logs/articulos"
    if not os.path.exists(carpeta):
        print("⚠️ No hay artículos procesados todavía.")
        return

    files = sorted(os.listdir(carpeta), reverse=True)
    data = []

    for file in files[:n]:
        path = os.path.join(carpeta, file)
        with open(path, encoding="utf-8") as f:
            art = json.load(f)
            data.append([
                art.get("titulo", "Sin título"),
                art.get("fecha", ""),
                art.get("autor", ""),
                "🟢" if art.get("resumen") else "❌",
                len(art.get("texto", "")),
                ", ".join(art.get("categorias", {}).keys())
            ])

    print(tabulate(data, headers=["Título", "Fecha", "Autor", "Resumen", "Longitud", "Categorías"]))

